# Compte-Rendu TP2 - DÃ©ploiement Kubernetes

**Date :** 2025-11-11
**Objectif :** MaÃ®triser le dÃ©ploiement d'applications sur Kubernetes avec les concepts de Compute, Storage et Network

---

## Table des matiÃ¨res

1. [PrÃ©requis et Configuration du Cluster](#1-prÃ©requis-et-configuration-du-cluster)
2. [Focus sur les commandes kubectl](#2-focus-sur-les-commandes-kubectl)
3. [Informations sur le cluster](#3-informations-sur-le-cluster)
4. [Les Objets Kubernetes](#4-les-objets-kubernetes)
5. [DÃ©ploiement de l'application VS Code](#5-dÃ©ploiement-de-lapplication-vs-code)
6. [DÃ©ploiement de l'application Guestbook](#6-dÃ©ploiement-de-lapplication-guestbook)
7. [Bilan et Conclusion](#7-bilan-et-conclusion)

---

## 1. PrÃ©requis et Configuration du Cluster

### 1.1 Configuration du Cluster Kind

Le TP nÃ©cessite un cluster Kind avec **2 control-plane + 1 worker**, avec forward des ports 80 et 443.

**Fichier de configuration** : `cluster.yaml`

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "ingress-ready=true"
  extraPortMappings:
  - containerPort: 80
    hostPort: 80
    protocol: TCP
  - containerPort: 443
    hostPort: 443
    protocol: TCP
- role: control-plane
- role: worker
```

**CrÃ©ation du cluster :**

```bash
kind create cluster --name cluster-tp2 --config cluster.yaml
```

**VÃ©rification :**

```bash
kubectl get nodes
```

**RÃ©sultat obtenu :**

```
NAME                        STATUS   ROLES           AGE     VERSION
cluster-tp2-control-plane   Ready    control-plane   3m47s   v1.27.3
cluster-tp2-control-plane2  Ready    control-plane   3m30s   v1.27.3
cluster-tp2-worker          Ready    <none>          2m34s   v1.27.3
```

### 1.2 Installation de l'Ingress Controller

Pour permettre l'accÃ¨s aux applications via HTTP, nous devons installer **nginx-ingress** :

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml
```

**Attendre que le pod ingress soit prÃªt :**

```bash
kubectl wait --namespace ingress-nginx \
  --for=condition=ready pod \
  --selector=app.kubernetes.io/component=controller \
  --timeout=120s
```

### 1.3 âš ï¸ ProblÃ¨me RencontrÃ© : Ingress Controller sur le Mauvais NÅ“ud

**SymptÃ´me :** Impossible d'accÃ©der aux applications via `http://localhost`

**Diagnostic :**

```bash
# VÃ©rifier oÃ¹ tourne l'ingress controller
kubectl get pods -n ingress-nginx -o wide
```

**RÃ©sultat initial :**
```
NAME                                        READY   STATUS    NODE
ingress-nginx-controller-6bc8c55c76-twb54   1/1     Running   cluster-tp2-control-plane2
```

**ProblÃ¨me identifiÃ© :**
- L'ingress controller tourne sur `cluster-tp2-control-plane2`
- Mais le port forwarding 80:80 est configurÃ© uniquement sur `cluster-tp2-control-plane`
- Le label `ingress-ready=true` est sur `cluster-tp2-control-plane`

**Solution :** Forcer l'ingress controller Ã  tourner sur le bon nÅ“ud avec un `nodeSelector`

```bash
# Patcher le deployment pour ajouter un nodeSelector
kubectl patch deployment -n ingress-nginx ingress-nginx-controller \
  -p '{"spec":{"template":{"spec":{"nodeSelector":{"ingress-ready":"true"}}}}}'
```

**VÃ©rification aprÃ¨s correction :**

```bash
kubectl get pods -n ingress-nginx -o wide
```

**RÃ©sultat :**
```
NAME                                       READY   STATUS    NODE
ingress-nginx-controller-bbd9ffff9-m454z   1/1     Running   cluster-tp2-control-plane
```

âœ… L'ingress controller tourne maintenant sur le bon nÅ“ud !

**Test de connectivitÃ© :**

```bash
# Test VS Code
curl -H "Host: mon-app.local" http://localhost
# RÃ©sultat : Found. Redirecting to ./login

# Test Guestbook
curl -H "Host: guestbook.local" http://localhost
# RÃ©sultat : <html ng-app="redis">...
```

**LeÃ§on apprise :** Dans un cluster multi-control-plane avec Kind :
- Le port forwarding n'est actif que sur le nÅ“ud avec `extraPortMappings`
- L'ingress controller **doit** tourner sur ce nÅ“ud spÃ©cifique
- Utiliser `nodeSelector` ou `nodeName` pour garantir le placement correct

### 1.4 ğŸ’¡ Meilleure Approche : Kustomize

Au lieu de patcher manuellement aprÃ¨s chaque installation, on peut utiliser **Kustomize** pour appliquer le `nodeSelector` automatiquement.

**CrÃ©er** : `ingress-kustomize/kustomization.yaml`

```yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml

patches:
  - patch: |-
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: ingress-nginx-controller
        namespace: ingress-nginx
      spec:
        template:
          spec:
            nodeSelector:
              ingress-ready: "true"
```

**Installer avec Kustomize :**

```bash
kubectl apply -k ingress-kustomize/
```

âœ… Le nodeSelector est automatiquement appliquÃ© dÃ¨s l'installation !

**Script d'automatisation complet** : [setup-cluster.sh](setup-cluster.sh)

```bash
#!/bin/bash
# CrÃ©e le cluster + installe ingress avec nodeSelector en une seule commande
./setup-cluster.sh
```

---

## 2. Focus sur les commandes kubectl

### 2.1 Les verbes kubectl les plus courants

| Verbe | Description |
|-------|-------------|
| `get` | Affiche une ressource |
| `describe` | Affiche des dÃ©tails spÃ©cifiques sur une ou plusieurs ressources |
| `create` | CrÃ©e une ressource (ou Ã  partir d'un fichier avec -f) |
| `apply` | Applique un manifeste |
| `delete` | Supprime une ressource (ou un fichier avec -f) |
| `logs` | Affiche les logs d'un pod |
| `exec` | ExÃ©cute une commande dans un conteneur |

### 2.2 Exemples de commandes utiles

```bash
# Lister tous les objets dans tous les namespaces
kubectl get all --all-namespaces

# Obtenir des informations dÃ©taillÃ©es sur un pod
kubectl describe pod <nom-du-pod>

# Voir les logs d'un deployment
kubectl logs -f deployment/<nom-deployment>

# ExÃ©cuter une commande dans un pod
kubectl exec -it <nom-pod> -- /bin/bash
```

---

## 3. Informations sur le cluster

### ğŸ“ Question 1 : Afficher la liste des namespaces du cluster

**Commande :**

```bash
kubectl get namespaces
```

**RÃ©sultat :**

```
NAME                 STATUS   AGE
default              Active   3m47s
ingress-nginx        Active   2m24s
kube-node-lease      Active   3m47s
kube-public          Active   3m47s
kube-system          Active   3m47s
local-path-storage   Active   3m35s
```

**RÃ©ponse :** Le cluster contient **6 namespaces**.

---

### ğŸ“ Question 2 : Afficher la liste des objets dans le namespace kube-system

**Commande :**

```bash
kubectl get all -n kube-system
```

**Objets affichÃ©s :**

- **Pods** : coredns, etcd, kube-apiserver, kube-controller-manager, kube-proxy, kube-scheduler, kindnet
- **Services** : kube-dns
- **DaemonSets** : kindnet, kube-proxy
- **Deployments** : coredns
- **ReplicaSets** : coredns

Ces objets constituent les composants essentiels du plan de contrÃ´le Kubernetes.

---

### ğŸ“ Question 3 : Quelle adresse IP le service kubernetes a-t-il ?

**Commande :**

```bash
kubectl get svc kubernetes
```

**RÃ©sultat :**

```
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   5m
```

**RÃ©ponse :** Le service kubernetes a l'adresse IP **10.96.0.1** (ClusterIP).

---

## 4. Les Objets Kubernetes

### 4.1 Afficher le service kubernetes en YAML

**Commande :**

```bash
kubectl get svc kubernetes -o yaml
```

**Extrait du rÃ©sultat :**

```yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    component: apiserver
    provider: kubernetes
  name: kubernetes
  namespace: default
spec:
  clusterIP: 10.96.0.1
  ports:
  - name: https
    port: 443
    protocol: TCP
    targetPort: 6443
  type: ClusterIP
```

### ğŸ“ Questions sur l'objet kubernetes :

**Q1. Quelle est la version de l'API de l'objet kubernetes ?**
**RÃ©ponse :** `v1`

**Q2. Quel est le type d'objet ?**
**RÃ©ponse :** `Service`

**Q3. Quelles sont les labels de cet objet ?**
**RÃ©ponse :**
- `component: apiserver`
- `provider: kubernetes`

---

### 4.2 API Resources : namespaced vs non-namespaced

**Commande pour lister les ressources :**

```bash
kubectl api-resources --verbs=list
```

### ğŸ“ Question : DiffÃ©rence entre api-resources namespaced true/false

**RÃ©ponse :**

**Namespaced=true** : Ces ressources sont **limitÃ©es Ã  un namespace** spÃ©cifique. Elles permettent d'isoler les ressources par projet/environnement.

**Exemples :**
- `Pod` - Les pods sont dÃ©ployÃ©s dans un namespace
- `Service` - Les services sont isolÃ©s par namespace
- `Deployment` - Les dÃ©ploiements appartiennent Ã  un namespace
- `ConfigMap`, `Secret` - Configuration isolÃ©e par namespace

**Namespaced=false** : Ces ressources sont **au niveau du cluster** et accessibles globalement.

**Exemples :**
- `Node` - Les nÅ“uds sont une ressource cluster-wide
- `Namespace` - Les namespaces eux-mÃªmes sont cluster-wide
- `PersistentVolume` - Les volumes persistants sont partagÃ©s au niveau cluster
- `ClusterRole` - Les rÃ´les RBAC au niveau cluster

**Avantage du namespacing :** Permet l'isolation multi-tenant, la gestion des quotas par Ã©quipe, et l'organisation logique des applications.

---

## 5. DÃ©ploiement de l'application VS Code

L'application **VS Code Server** illustre les 3 composants principaux d'une application cloud dans Kubernetes :

1. **Compute** (Deployment)
2. **Storage** (PersistentVolumeClaim)
3. **Network** (Service + Ingress)

### 5.1 Compute Manifest - `compute.yaml`

Ce manifeste dÃ©ploie un **Deployment** qui gÃ¨re un pod VS Code Server.

```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: code-server
  name: code-server
spec:
  selector:
    matchLabels:
      app: code-server
  replicas: 1
  template:
    metadata:
      labels:
        app: code-server
    spec:
      containers:
      - env:
        - name: PASSWORD
          valueFrom:
            secretKeyRef:
              name: coder-password
              key: password
        image: codercom/code-server:latest
        imagePullPolicy: Always
        name: code-server
        ports:
        - name: code-server
          containerPort: 8080
          protocol: TCP
        volumeMounts:
        - mountPath: /home/coder
          name: coder
      initContainers:
      - name: pvc-permission-fix
        image: busybox
        command: ["/bin/chmod","-R","777", "/home/coder"]
        volumeMounts:
        - name: coder
          mountPath: /home/coder
      volumes:
      - name: coder
        persistentVolumeClaim:
          claimName: code-server
```

### ğŸ“ Questions sur le Deployment :

**Q1. Ã€ quoi sert la section `env` ?**

**RÃ©ponse :** La section `env` permet de dÃ©finir des **variables d'environnement** pour le conteneur. Dans cet exemple :
- La variable `PASSWORD` est injectÃ©e dans le conteneur
- Elle provient d'un **Secret Kubernetes** (`coder-password`)
- Cela permet de ne pas stocker le mot de passe en clair dans le manifeste
- Le conteneur VS Code utilise cette variable pour protÃ©ger l'accÃ¨s

**Q2. Ã€ quoi sert la section `volume` et `volumeMount` ?**

**RÃ©ponse :**

**`volumes`** : DÃ©clare les volumes disponibles pour le pod
- Dans notre cas, un volume `coder` qui rÃ©fÃ©rence un `PersistentVolumeClaim`
- Le PVC fournit un stockage persistant qui survit au redÃ©marrage du pod

**`volumeMounts`** : Monte un volume dans le systÃ¨me de fichiers du conteneur
- Le volume `coder` est montÃ© dans `/home/coder`
- Cela permet de persister les fichiers et configurations de l'utilisateur
- Sans cela, toutes les modifications seraient perdues au redÃ©marrage du pod

**`initContainers`** : Conteneur qui s'exÃ©cute avant le conteneur principal
- Ici, il corrige les permissions du volume (chmod 777)
- NÃ©cessaire car le PVC peut avoir des permissions restrictives par dÃ©faut

---

### 5.2 Storage Manifest - `storage.yaml`

```yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: code-server
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
```

### ğŸ“ Question : Pourquoi crÃ©er un PVC ?

**RÃ©ponse :**

Un **PersistentVolumeClaim (PVC)** est crÃ©Ã© pour plusieurs raisons :

1. **Persistance des donnÃ©es** : Les conteneurs sont Ã©phÃ©mÃ¨res. Sans PVC, toutes les donnÃ©es seraient perdues au redÃ©marrage du pod.

2. **Abstraction du stockage** : Le PVC abstrait le stockage sous-jacent :
   - L'application demande simplement "5Gi de stockage"
   - Kubernetes se charge de provisionner le volume via la **StorageClass**
   - Le type de stockage (NFS, iSCSI, cloud provider) est transparent pour l'application

3. **PortabilitÃ©** : Le mÃªme manifeste fonctionne sur diffÃ©rents clusters :
   - Sur AWS â†’ EBS volume
   - Sur GCP â†’ Persistent Disk
   - Sur Kind â†’ local-path provisioner

4. **Gestion du cycle de vie** : Le volume persiste mÃªme si le pod est supprimÃ© (selon la `reclaimPolicy`)

5. **Isolation** : Chaque application a son propre espace de stockage isolÃ©

**Modes d'accÃ¨s disponibles :**
- `ReadWriteOnce` (RWO) : Montable en lecture-Ã©criture par un seul nÅ“ud
- `ReadOnlyMany` (ROX) : Montable en lecture seule par plusieurs nÅ“uds
- `ReadWriteMany` (RWX) : Montable en lecture-Ã©criture par plusieurs nÅ“uds

---

### 5.3 Network Manifest - `network.yaml`

Ce manifeste contient **2 objets** : un Service et un Ingress.

```yaml
---
apiVersion: v1
kind: Service
metadata:
  name: code-server
  labels:
    app: code-server
spec:
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 8080
  selector:
    app: code-server
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: code-server
  labels:
    app: code-server
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: "mon-app.local"
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: code-server
            port:
              number: 8080
```

**RÃ´le du Service :**
- Expose le port 8080 du pod
- Fournit une IP stable (ClusterIP)
- Load-balance entre les pods avec le label `app: code-server`

**RÃ´le de l'Ingress :**
- Point d'entrÃ©e HTTP/HTTPS depuis l'extÃ©rieur du cluster
- Route le trafic vers le service `code-server`
- Permet l'accÃ¨s via un nom de domaine (`mon-app.local`)

---

### 5.4 Secret Kubernetes - `secret.yaml`

Pour sÃ©curiser le mot de passe, nous utilisons un **Secret** :

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: coder-password
type: Opaque
stringData:
  password: MonSuperMotDePasse123
```

### ğŸ“ Question : Comment faire en sorte que ce secret ne soit pas en clair dans nos manifests ?

**RÃ©ponse :**

Le stockage en base64 (par dÃ©faut dans Kubernetes) **n'est PAS du chiffrement** ! Voici les solutions recommandÃ©es :

**1. Sealed Secrets (Bitnami)**
```bash
# Installer sealed-secrets controller
kubectl apply -f https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.18.0/controller.yaml

# Chiffrer un secret
kubeseal --format=yaml < secret.yaml > sealed-secret.yaml
```
- Le secret chiffrÃ© peut Ãªtre committÃ© dans Git
- Seul le cluster peut le dÃ©chiffrer

**2. External Secrets Operator**
- IntÃ©gration avec des gestionnaires de secrets externes :
  - AWS Secrets Manager
  - Azure Key Vault
  - HashiCorp Vault
  - Google Secret Manager
- Les secrets ne sont jamais stockÃ©s dans Git

**3. SOPS (Secrets OPerationS)**
```bash
# Chiffrer un fichier avec SOPS
sops --encrypt --age <public-key> secret.yaml > secret.enc.yaml
```
- Chiffrement avec GPG ou age
- Compatible avec Git et CI/CD

**4. HashiCorp Vault**
- Solution entreprise complÃ¨te
- Rotation automatique des secrets
- Audit trail complet

**5. Kubernetes RBAC + Git privÃ©**
- Stocker les secrets dans un repo Git **privÃ©** sÃ©parÃ©
- Restreindre l'accÃ¨s via RBAC
- Ne jamais committer les secrets dans le repo applicatif

**Meilleure pratique :** External Secrets Operator + cloud provider secret manager

---

### 5.5 DÃ©ploiement de VS Code

```bash
# DÃ©ployer tous les manifests
kubectl apply -f vs_code/

# VÃ©rifier le dÃ©ploiement
kubectl get pods
kubectl get pvc
kubectl get svc
kubectl get ingress
```

**RÃ©sultat :**

```
NAME                           READY   STATUS    RESTARTS   AGE
code-server-7ddb4bdd54-4wzrs   1/1     Running   0          2m

NAME          STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
code-server   Bound    pvc-xxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx      5Gi        RWO            standard       2m

NAME          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
code-server   ClusterIP   10.96.179.65    <none>        8080/TCP   2m

NAME          CLASS    HOSTS           ADDRESS     PORTS   AGE
code-server   <none>   mon-app.local   localhost   80      2m
```

### ğŸ“ Questions de test :

**Q1. Comment accÃ©dez-vous Ã  l'application mon-app.local ?**

**RÃ©ponse :**

1. **Ajouter une entrÃ©e dans /etc/hosts :**

```bash
echo "127.0.0.1 mon-app.local" | sudo tee -a /etc/hosts
```

2. **AccÃ©der via le navigateur :**

```
http://mon-app.local
```

3. **Test avec curl :**

```bash
curl -H "Host: mon-app.local" http://localhost
```

**Explication :**
- Kind forward le port 80 du conteneur control-plane vers localhost:80
- L'Ingress controller nginx route le trafic selon l'en-tÃªte Host
- Le navigateur rÃ©sout `mon-app.local` vers 127.0.0.1 grÃ¢ce Ã  /etc/hosts

---

**Q2. Comment affichez-vous les logs des requÃªtes entrantes sur votre application ?**

**RÃ©ponse :**

**Logs du pod VS Code :**
```bash
kubectl logs -f deployment/code-server
```

**Logs de l'Ingress Controller (requÃªtes HTTP) :**
```bash
kubectl logs -f -n ingress-nginx deployment/ingress-nginx-controller
```

**Logs en temps rÃ©el avec grep :**
```bash
kubectl logs -f -n ingress-nginx deployment/ingress-nginx-controller | grep "mon-app.local"
```

**Afficher les derniÃ¨res 100 lignes :**
```bash
kubectl logs --tail=100 deployment/code-server
```

---

**Q3. Quand vous supprimez le pod que se passe-t-il ?**

**RÃ©ponse :**

**Test :**
```bash
# Supprimer le pod
kubectl delete pod code-server-7ddb4bdd54-4wzrs

# Observer la recrÃ©ation automatique
kubectl get pods -w
```

**RÃ©sultat observÃ© :**

```
NAME                           READY   STATUS        RESTARTS   AGE
code-server-7ddb4bdd54-4wzrs   1/1     Terminating   0          5m
code-server-7ddb4bdd54-xk9nm   0/1     Pending       0          0s
code-server-7ddb4bdd54-xk9nm   0/1     ContainerCreating   0     1s
code-server-7ddb4bdd54-xk9nm   1/1     Running             0     15s
```

**Explication :**

1. **Le Deployment maintient l'Ã©tat dÃ©sirÃ©** : `replicas: 1`
2. Quand le pod est supprimÃ©, le **ReplicaSet** (contrÃ´lÃ© par le Deployment) dÃ©tecte que l'Ã©tat actuel (0 pod) â‰  Ã©tat dÃ©sirÃ© (1 pod)
3. Il crÃ©e **automatiquement un nouveau pod** avec un nouveau nom
4. Le nouveau pod :
   - Remonte le mÃªme **PVC** (les donnÃ©es sont prÃ©servÃ©es)
   - RÃ©cupÃ¨re le mÃªme **Secret**
   - Est accessible via le mÃªme **Service** (grÃ¢ce aux labels)

**Auto-rÃ©paration (self-healing)** : C'est un principe fondamental de Kubernetes !

---

## 6. DÃ©ploiement de l'application Guestbook

L'application **Guestbook** est une application PHP avec Redis qui illustre une architecture multi-tiers.

### 6.1 Architecture de Guestbook

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚  3 replicas (PHP)
â”‚   (PHP)         â”‚  Port: 80
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
    â”‚          â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”   â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚Redis â”‚   â”‚ Redis  â”‚
â”‚Leaderâ”‚   â”‚Followerâ”‚ 2 replicas
â”‚(RW)  â”‚   â”‚  (RO)  â”‚
â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Composants :**
- **Frontend** : 3 rÃ©plicas PHP qui affichent/enregistrent des messages
- **Redis Leader** : 1 instance pour les Ã©critures
- **Redis Followers** : 2 instances pour les lectures (rÃ©plication)

---

### 6.2 DÃ©ploiement Redis Leader

**Fichier** : `redis-leader-deployment.yaml`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-leader
  labels:
    app: redis
    role: leader
    tier: backend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
        role: leader
        tier: backend
    spec:
      containers:
      - name: leader
        image: "docker.io/redis:6.0.5"
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379
```

**Service Redis Leader** : `redis-leader-service.yaml`

```yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-leader
  labels:
    app: redis
    role: leader
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: leader
    tier: backend
```

**DÃ©ploiement :**

```bash
kubectl apply -f guestbook/redis-leader-deployment.yaml
kubectl apply -f guestbook/redis-leader-service.yaml

# VÃ©rification
kubectl get pods -l app=redis,role=leader
kubectl logs -f deployment/redis-leader
```

---

### 6.3 DÃ©ploiement Redis Followers

**Fichier** : `redis-follower-deployment.yaml`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-follower
  labels:
    app: redis
    role: follower
    tier: backend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
        role: follower
        tier: backend
    spec:
      containers:
      - name: follower
        image: gcr.io/google_samples/gb-redis-follower:v2
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379
```

**Service Redis Followers** : `redis-follower-service.yaml`

```yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-follower
  labels:
    app: redis
    role: follower
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: follower
    tier: backend
```

**DÃ©ploiement :**

```bash
kubectl apply -f guestbook/redis-follower-deployment.yaml
kubectl apply -f guestbook/redis-follower-service.yaml

# VÃ©rification
kubectl get pods -l app=redis,role=follower
```

**RÃ©sultat attendu :**

```
NAME                              READY   STATUS    RESTARTS   AGE
redis-follower-6f6cd6cbdb-kn6b8   1/1     Running   0          1m
redis-follower-6f6cd6cbdb-rx9kd   1/1     Running   0          1m
```

---

### 6.4 DÃ©ploiement du Frontend PHP

**Fichier** : `frontend-deployment.yaml`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v5
        env:
        - name: GET_HOSTS_FROM
          value: "dns"
        resources:
          requests:
            cpu: 10m
            memory: 10Mi
        ports:
        - containerPort: 80
```

**Explication de l'environnement :**
- `GET_HOSTS_FROM: "dns"` : L'application utilise le DNS Kubernetes pour trouver les services `redis-leader` et `redis-follower`
- Le frontend Ã©crit dans `redis-leader.default.svc.cluster.local`
- Le frontend lit depuis `redis-follower.default.svc.cluster.local`

**Service Frontend** : `frontend-service.yaml`

```yaml
apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend
```

**Ingress Frontend** : `frontend-ingress.yaml`

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: "guestbook.local"
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend
            port:
              number: 80
```

**DÃ©ploiement :**

```bash
kubectl apply -f guestbook/frontend-deployment.yaml
kubectl apply -f guestbook/frontend-service.yaml
kubectl apply -f guestbook/frontend-ingress.yaml

# VÃ©rification
kubectl get pods -l app=guestbook
kubectl get svc frontend
kubectl get ingress frontend
```

---

### 6.5 Test de l'application Guestbook

**1. Ajouter l'entrÃ©e DNS locale :**

```bash
echo "127.0.0.1 guestbook.local" | sudo tee -a /etc/hosts
```

**2. AccÃ©der Ã  l'application :**

```
http://guestbook.local
```

**3. Tester l'application :**

- Ajouter un message dans le champ de texte
- Cliquer sur "Submit"
- Le message doit apparaÃ®tre dans la liste

**4. VÃ©rifier le fonctionnement de Redis :**

```bash
# VÃ©rifier les logs du leader
kubectl logs -f deployment/redis-leader

# VÃ©rifier les logs des followers
kubectl logs -f deployment/redis-follower

# Se connecter Ã  Redis pour vÃ©rifier les donnÃ©es
kubectl exec -it deployment/redis-leader -- redis-cli
> KEYS *
> GET messages
```

**5. Tester la haute disponibilitÃ© :**

```bash
# Supprimer un pod frontend
kubectl delete pod -l app=guestbook,tier=frontend --field-selector status.phase=Running

# Observer la recrÃ©ation automatique
kubectl get pods -l app=guestbook -w

# L'application reste accessible pendant la recrÃ©ation !
curl http://guestbook.local
```

---

## 7. Bilan et Conclusion

### 7.1 Concepts Kubernetes MaÃ®trisÃ©s

**âœ… Objets de base :**
- **Deployment** : Gestion dÃ©clarative des Pods
- **Service** : Exposition stable des Pods (load-balancing)
- **Ingress** : Routage HTTP/HTTPS externe
- **PersistentVolumeClaim** : Stockage persistant
- **Secret** : Gestion sÃ©curisÃ©e des credentials

**âœ… Patterns architecturaux :**
- **Compute, Storage, Network** : Les 3 piliers du cloud
- **Architecture multi-tiers** : Frontend + Backend (Redis)
- **Haute disponibilitÃ©** : RÃ©plication avec leader/follower
- **Auto-rÃ©paration** : Self-healing via Deployment controller

**âœ… Commandes kubectl :**
```bash
# Gestion des ressources
kubectl get/describe/apply/delete

# Debug
kubectl logs -f deployment/<name>
kubectl exec -it <pod> -- /bin/bash

# Supervision
kubectl get pods -w
kubectl get events
```

---

### 7.2 Architecture DÃ©ployÃ©e

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Ingress Controller              â”‚
â”‚       (nginx - Port 80/443)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚             â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚mon-app.localâ”‚   â”‚guestbook.localâ”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚            â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚Service:       â”‚   â”‚Service:     â”‚
     â”‚code-server    â”‚   â”‚frontend     â”‚
     â”‚Port: 8080     â”‚   â”‚Port: 80     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚            â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚Deployment:    â”‚   â”‚Deployment:  â”‚
     â”‚code-server    â”‚   â”‚frontend     â”‚
     â”‚Replicas: 1    â”‚   â”‚Replicas: 3  â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚            â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚PVC:           â”‚   â”‚Redis Leader â”‚
     â”‚code-server    â”‚   â”‚+ Followers  â”‚
     â”‚5Gi            â”‚   â”‚             â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 7.3 Commandes de Gestion du Cluster

**Lister toutes les ressources dÃ©ployÃ©es :**

```bash
kubectl get all
```

**VÃ©rifier l'Ã©tat complet :**

```bash
# Pods
kubectl get pods -o wide

# Services
kubectl get svc

# Ingress
kubectl get ingress

# PVC et PV
kubectl get pvc
kubectl get pv

# Events rÃ©cents
kubectl get events --sort-by=.metadata.creationTimestamp
```

**Nettoyer les ressources :**

```bash
# Supprimer l'application VS Code
kubectl delete -f vs_code/

# Supprimer Guestbook
kubectl delete -f guestbook/

# Supprimer le cluster
kind delete cluster --name cluster-tp2
```

---

### 7.4 Points ClÃ©s Ã  Retenir

**1. DÃ©claratif vs ImpÃ©ratif**
- Kubernetes favorise l'approche dÃ©clarative (manifests YAML)
- L'Ã©tat dÃ©sirÃ© est maintenu automatiquement par les controllers

**2. Labels et Selectors**
- Les Services trouvent les Pods via les labels
- Les Deployments gÃ¨rent les Pods via matchLabels
- StratÃ©gie de labelling cohÃ©rente = critique

**3. Namespaces**
- Isolation logique des ressources
- Quotas et RBAC par namespace
- Ne jamais dÃ©ployer en production dans `default`

**4. Stockage**
- PVC = abstraction portable
- StorageClass = provisionneur dynamique
- Les donnÃ©es survivent aux pods

**5. Networking**
- Service = IP stable + load-balancing
- Ingress = reverse proxy HTTP/HTTPS
- DNS interne : `<service>.<namespace>.svc.cluster.local`

**6. SÃ©curitÃ©**
- Secrets pour les credentials
- RBAC pour les accÃ¨s
- Network Policies pour l'isolation rÃ©seau
- Pod Security Standards

**7. Haute DisponibilitÃ© du Control-Plane**
- **2 control-planes** dans ce TP pour apprendre les concepts HA
- Chaque control-plane exÃ©cute : kube-apiserver, etcd, controller-manager, scheduler
- **kube-apiserver** : Mode Actif-Actif (les 2 rÃ©pondent simultanÃ©ment)
- **etcd** : Cluster distribuÃ© avec consensus Raft (quorum nÃ©cessaire)
- **controller-manager & scheduler** : Mode Actif-Passif (Ã©lection de leader)
- **Production** : RecommandÃ© 3 ou 5 control-planes (nombre impair) pour un quorum optimal
- **Avantage** : TolÃ©rance aux pannes, zero downtime, disaster recovery

**Composants etcd vÃ©rifiÃ©s :**
```bash
kubectl exec -n kube-system etcd-cluster-tp2-control-plane -- etcdctl member list
# RÃ©sultat : 2 membres (cluster-tp2-control-plane et cluster-tp2-control-plane2)
```

**Leader election vÃ©rifiÃ©e :**
```bash
kubectl get lease -n kube-system kube-controller-manager
# holderIdentity: cluster-tp2-control-plane
```

**Pourquoi pas optimal avec 2 ?**
- Quorum etcd nÃ©cessite 2/2 membres actifs
- Si 1 tombe â†’ cluster inaccessible (quorum non atteint)
- Meilleur choix production : **3 control-planes** (tolÃ¨re 1 panne)

---

### 7.5 AmÃ©liorations Possibles

**Pour aller plus loin :**

1. **ObservabilitÃ©**
   - DÃ©ployer Prometheus + Grafana
   - Configurer des dashboards de monitoring
   - Alerting avec AlertManager

2. **Haute DisponibilitÃ©**
   - Pod Disruption Budgets
   - Anti-affinity rules (spread pods across nodes)
   - Health checks (liveness/readiness probes)

3. **SÃ©curitÃ© AvancÃ©e**
   - Network Policies pour isoler les tiers
   - Pod Security Admission
   - Secrets encryption at rest
   - External Secrets Operator

4. **CI/CD**
   - GitOps avec ArgoCD ou Flux
   - Automated deployments
   - Blue/Green ou Canary deployments

5. **ScalabilitÃ©**
   - Horizontal Pod Autoscaler (HPA)
   - Vertical Pod Autoscaler (VPA)
   - Cluster Autoscaler

---

### 7.6 Ressources Utiles

**Documentation officielle :**
- https://kubernetes.io/docs/
- https://kind.sigs.k8s.io/

**Cheat sheets :**
- kubectl : https://kubernetes.io/docs/reference/kubectl/cheatsheet/

**Tutoriels :**
- https://kubernetes.io/docs/tutorials/
- https://www.katacoda.com/courses/kubernetes

---

## Annexe : RÃ©capitulatif des Commandes

```bash
# Cluster Kind
kind create cluster --name cluster-tp2 --config cluster.yaml
kind get clusters
kind delete cluster --name cluster-tp2

# Ingress-nginx
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml
kubectl wait --namespace ingress-nginx --for=condition=ready pod --selector=app.kubernetes.io/component=controller --timeout=120s

# DÃ©ploiements
kubectl apply -f vs_code/
kubectl apply -f guestbook/

# VÃ©rifications
kubectl get nodes
kubectl get namespaces
kubectl get all
kubectl get pods -o wide
kubectl get svc
kubectl get ingress
kubectl get pvc

# Debug
kubectl describe pod <pod-name>
kubectl logs -f deployment/<deployment-name>
kubectl exec -it <pod-name> -- /bin/bash
kubectl get events --sort-by=.metadata.creationTimestamp

# Tests
curl -H "Host: mon-app.local" http://localhost
curl -H "Host: guestbook.local" http://localhost

# Nettoyage
kubectl delete -f vs_code/
kubectl delete -f guestbook/
kind delete cluster --name cluster-tp2
```

---

**Fin du Compte-Rendu TP2**

**Date de rÃ©alisation :** 2025-11-11
**Cluster utilisÃ© :** Kind v0.20.0 avec Kubernetes v1.27.3
**Applications dÃ©ployÃ©es :** VS Code Server + Guestbook PHP/Redis
