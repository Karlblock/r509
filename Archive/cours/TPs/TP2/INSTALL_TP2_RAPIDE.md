# Installation Rapide - TP02 : D√©ploiement d'applications sur Kubernetes

Guide pratique pour r√©aliser le TP02 - D√©ploiement d'applications sur Kubernetes.

## Pr√©requis

- ‚úÖ TP01 termin√© (cluster Kind fonctionnel)
- ‚úÖ 4 CPUs (2 c≈ìurs control-plane + 2 c≈ìurs worker)
- ‚úÖ Cluster Kind avec 2 control-plane + 1 worker
- ‚úÖ Port forwarding 80:80 et 443:443 configur√©
- ‚úÖ Ingress Controller NGINX d√©ploy√©
- ‚úÖ Nginx de test d√©ploy√© et accessible
- ‚úÖ **Proxy d√©sactiv√©** pour Kubernetes (`proxy-off`)

Si vous n'avez pas fait le TP01, consultez [INSTALL_TP1_RAPIDE.md](INSTALL_TP1_RAPIDE.md)

## ‚ö†Ô∏è Rappel Important sur le Proxy

**AVANT de commencer le TP02**, assurez-vous que le proxy est d√©sactiv√© :

```bash
# V√©rifier l'√©tat du proxy
proxy-status

# Si un proxy appara√Æt, le d√©sactiver
proxy-off

# V√©rifier √† nouveau (ne devrait RIEN afficher)
env | grep -i proxy
```

**Configuration recommand√©e** :
- ‚úÖ Proxy Docker daemon : **ACTIV√â** (pour t√©l√©charger les images)
- ‚ùå Variables proxy shell : **D√âSACTIV√âES** (pour kubectl/Kind)

```bash
# V√©rifier le proxy Docker (devrait √™tre configur√©)
sudo systemctl show --property=Environment docker

# V√©rifier les variables d'environnement (ne devrait RIEN afficher)
env | grep -i proxy
```

## Structure du TP02

Le TP02 couvre deux d√©ploiements :
1. **VS Code Server** - Application simple avec stockage persistant
2. **Guestbook PHP/Redis** - Application multi-tiers compl√®te

---

## Partie 1 : D√©ploiement VS Code Server

### √âtape 1 : Cr√©er le namespace et le dossier de travail

```bash
# Cr√©er un namespace d√©di√©
kubectl create namespace tp-kubernetes

# D√©finir comme namespace par d√©faut
kubectl config set-context --current --namespace=tp-kubernetes

# V√©rifier
kubectl config view --minify | grep namespace:

# Cr√©er le dossier de travail
mkdir -p ~/tp02/vs_code
cd ~/tp02/vs_code
```

### √âtape 2 : Cr√©er le manifest Compute (Deployment)

Cr√©ez le fichier `compute.yaml` :

```bash
cat > compute.yaml <<'EOF'
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: code-server
  name: code-server
spec:
  selector:
    matchLabels:
      app: code-server
  replicas: 1
  template:
    metadata:
      labels:
        app: code-server
    spec:
      containers:
      - env:
        - name: PASSWORD
          valueFrom:
            secretKeyRef:
              name: coder-password
              key: password
        image: codercom/code-server:latest
        imagePullPolicy: Always
        name: code-server
        ports:
        - name: code-server
          containerPort: 8080
          protocol: TCP
        volumeMounts:
        - mountPath: /home/coder
          name: coder
      initContainers:
      - name: pvc-permission-fix
        image: busybox
        command: ["/bin/chmod","-R","777", "/home/coder"]
        volumeMounts:
        - name: coder
          mountPath: /home/coder
      volumes:
      - name: coder
        persistentVolumeClaim:
          claimName: code-server
EOF
```

**Points cl√©s** :
- `env` : Variables d'environnement (PASSWORD vient d'un Secret)
- `volumeMounts` : Monte le PVC dans `/home/coder`
- `initContainers` : Fixe les permissions avant le d√©marrage

### √âtape 3 : Cr√©er le manifest Storage (PVC)

Cr√©ez le fichier `storage.yaml` :

```bash
cat > storage.yaml <<'EOF'
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: code-server
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
EOF
```

**Points cl√©s** :
- Demande 5Gi de stockage
- Mode `ReadWriteOnce` : un seul n≈ìud peut monter le volume en lecture/√©criture
- Kind fournit automatiquement un PersistentVolume via StorageClass

### √âtape 4 : Cr√©er le manifest Network (Service + Ingress)

Cr√©ez le fichier `network.yaml` :

```bash
cat > network.yaml <<'EOF'
---
apiVersion: v1
kind: Service
metadata:
  name: code-server
  labels:
    app: code-server
spec:
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 8080
  selector:
    app: code-server
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: code-server
  labels:
    app: code-server
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: "mon-app.local"
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: code-server
            port:
              number: 8080
EOF
```

**Points cl√©s** :
- Service expose le pod sur le port 8080 en interne
- Ingress route le trafic de `mon-app.local` vers le service

### √âtape 5 : Cr√©er le Secret

Cr√©ez le fichier `secret.yaml` :

```bash
cat > secret.yaml <<'EOF'
---
apiVersion: v1
kind: Secret
metadata:
  name: coder-password
type: Opaque
stringData:
  password: MonSuperMotDePasse123
EOF
```

**‚ö†Ô∏è ATTENTION** : En production, ne jamais commiter les secrets en clair !
Solutions recommand√©es :
- **Sealed Secrets** (Bitnami)
- **External Secrets Operator**
- **HashiCorp Vault**
- **kubectl create secret** (cr√©e le secret sans fichier)

```bash
# Alternative : cr√©er le secret via kubectl (recommand√©)
kubectl create secret generic coder-password \
  --from-literal=password=MonSuperMotDePasse123 \
  -n tp-kubernetes
```

### √âtape 6 : D√©ployer l'application

```bash
# D√©ployer dans l'ordre
kubectl apply -f secret.yaml
kubectl apply -f storage.yaml
kubectl apply -f compute.yaml
kubectl apply -f network.yaml

# V√©rifier le d√©ploiement
kubectl get all
kubectl get pvc
kubectl get ingress
```

### √âtape 7 : Acc√©der √† l'application

```bash
# Ajouter l'entr√©e DNS locale
echo "127.0.0.1 mon-app.local" | sudo tee -a /etc/hosts

# Attendre que le pod soit pr√™t
kubectl wait --for=condition=ready pod -l app=code-server --timeout=120s

# V√©rifier les logs
kubectl logs -l app=code-server --tail=50

# Tester l'acc√®s
curl -I http://mon-app.local
```

Ouvrez votre navigateur : [http://mon-app.local](http://mon-app.local)

**Mot de passe** : `MonSuperMotDePasse123`

### √âtape 8 : Questions du TP

#### Q1 : Comment acc√©der √† l'application mon-app.local ?
```bash
# Via l'entr√©e dans /etc/hosts qui pointe vers 127.0.0.1
# L'Ingress Controller √©coute sur le port 80 et route vers le service
```

#### Q2 : Comment afficher les logs des requ√™tes entrantes ?
```bash
# Logs du pod de l'application
kubectl logs -l app=code-server -f

# Logs de l'Ingress Controller
kubectl logs -n ingress-nginx -l app.kubernetes.io/component=controller -f
```

#### Q3 : Que se passe-t-il quand vous supprimez le pod ?
```bash
# Supprimer le pod
kubectl delete pod -l app=code-server

# Observer la recr√©ation automatique
kubectl get pods -w

# Le Deployment d√©tecte que le pod est manquant et en recr√©e un automatiquement
# C'est le principe du "self-healing" de Kubernetes
```

### √âtape 9 : Nettoyage

```bash
# Supprimer toutes les ressources
kubectl delete -f network.yaml
kubectl delete -f compute.yaml
kubectl delete -f storage.yaml
kubectl delete -f secret.yaml

# OU supprimer tout d'un coup
kubectl delete all,ingress,pvc,secret -l app=code-server
```

---

## Partie 2 : Application Guestbook PHP/Redis

### Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Frontend PHP   ‚îÇ  (3 replicas)
‚îÇ   Guestbook     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Redis‚îÇ  ‚îÇ   Redis   ‚îÇ
‚îÇLeader‚îÇ  ‚îÇ Followers ‚îÇ (2 replicas)
‚îÇ(√©criture)‚îÇ (lecture) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### √âtape 1 : Cr√©er le dossier de travail

```bash
mkdir -p ~/tp02/guestbook-php
cd ~/tp02/guestbook-php
```

### √âtape 2 : D√©ployer Redis Leader

Cr√©ez `redis-leader-deployment.yaml` :

```bash
cat > redis-leader-deployment.yaml <<'EOF'
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-leader
  labels:
    app: redis
    role: leader
    tier: backend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
        role: leader
        tier: backend
    spec:
      containers:
      - name: leader
        image: "docker.io/redis:6.0.5"
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379
EOF
```

Cr√©ez `redis-leader-service.yaml` :

```bash
cat > redis-leader-service.yaml <<'EOF'
---
apiVersion: v1
kind: Service
metadata:
  name: redis-leader
  labels:
    app: redis
    role: leader
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: leader
    tier: backend
EOF
```

D√©ployer :

```bash
kubectl apply -f redis-leader-deployment.yaml
kubectl apply -f redis-leader-service.yaml

# V√©rifier
kubectl get pods -l app=redis
kubectl logs -f deployment/redis-leader
```

### √âtape 3 : D√©ployer Redis Followers

Cr√©ez `redis-follower-deployment.yaml` :

```bash
cat > redis-follower-deployment.yaml <<'EOF'
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-follower
  labels:
    app: redis
    role: follower
    tier: backend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
        role: follower
        tier: backend
    spec:
      containers:
      - name: follower
        image: gcr.io/google_samples/gb-redis-follower:v2
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379
EOF
```

Cr√©ez `redis-follower-service.yaml` :

```bash
cat > redis-follower-service.yaml <<'EOF'
---
apiVersion: v1
kind: Service
metadata:
  name: redis-follower
  labels:
    app: redis
    role: follower
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: follower
    tier: backend
EOF
```

D√©ployer :

```bash
kubectl apply -f redis-follower-deployment.yaml
kubectl apply -f redis-follower-service.yaml

# V√©rifier
kubectl get pods -l role=follower
kubectl get service
```

### √âtape 4 : D√©ployer le Frontend Guestbook

Cr√©ez `frontend-deployment.yaml` :

```bash
cat > frontend-deployment.yaml <<'EOF'
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v5
        env:
        - name: GET_HOSTS_FROM
          value: "dns"
        resources:
          requests:
            cpu: 10m
            memory: 10Mi
        ports:
        - containerPort: 80
EOF
```

Cr√©ez `frontend-service.yaml` :

```bash
cat > frontend-service.yaml <<'EOF'
---
apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend
EOF
```

D√©ployer :

```bash
kubectl apply -f frontend-deployment.yaml
kubectl apply -f frontend-service.yaml

# V√©rifier
kubectl get pods -l app=guestbook -l tier=frontend
kubectl get service frontend
```

### √âtape 5 : Cr√©er l'Ingress pour Guestbook

Cr√©ez `frontend-ingress.yaml` :

```bash
cat > frontend-ingress.yaml <<'EOF'
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: guestbook-ingress
  labels:
    app: guestbook
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: "guestbook.local"
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend
            port:
              number: 80
EOF
```

D√©ployer :

```bash
kubectl apply -f frontend-ingress.yaml

# Ajouter l'entr√©e DNS
echo "127.0.0.1 guestbook.local" | sudo tee -a /etc/hosts

# V√©rifier
kubectl get ingress
```

### √âtape 6 : Tester l'application Guestbook

```bash
# Attendre que tous les pods soient pr√™ts
kubectl wait --for=condition=ready pod -l app=guestbook --timeout=120s
kubectl wait --for=condition=ready pod -l app=redis --timeout=120s

# V√©rifier l'√©tat complet
kubectl get all

# Tester
curl http://guestbook.local
```

Ouvrez votre navigateur : [http://guestbook.local](http://guestbook.local)

**Testez l'application** :
1. √âcrivez un message dans le champ de texte
2. Cliquez sur "Submit"
3. Le message devrait appara√Ætre dans la liste

### √âtape 7 : Tests de r√©silience

```bash
# Test 1 : Supprimer un pod Redis follower
kubectl delete pod -l role=follower --force --grace-period=0
kubectl get pods -l app=redis -w

# Test 2 : Supprimer un pod frontend
kubectl delete pod -l tier=frontend --force --grace-period=0
kubectl get pods -l app=guestbook -w

# Test 3 : Scaler le frontend
kubectl scale deployment frontend --replicas=5
kubectl get pods -l app=guestbook

# Test 4 : Revenir √† 3 replicas
kubectl scale deployment frontend --replicas=3
```

---

## D√©ploiement avec un seul fichier (Bonus)

Vous pouvez combiner tous les manifests dans un seul fichier :

```bash
cat > guestbook-complete.yaml <<'EOF'
---
# Redis Leader Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-leader
  labels:
    app: redis
    role: leader
    tier: backend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: leader
  template:
    metadata:
      labels:
        app: redis
        role: leader
        tier: backend
    spec:
      containers:
      - name: leader
        image: "docker.io/redis:6.0.5"
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379
---
# Redis Leader Service
apiVersion: v1
kind: Service
metadata:
  name: redis-leader
  labels:
    app: redis
    role: leader
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: leader
    tier: backend
---
# Redis Follower Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-follower
  labels:
    app: redis
    role: follower
    tier: backend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: follower
  template:
    metadata:
      labels:
        app: redis
        role: follower
        tier: backend
    spec:
      containers:
      - name: follower
        image: gcr.io/google_samples/gb-redis-follower:v2
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379
---
# Redis Follower Service
apiVersion: v1
kind: Service
metadata:
  name: redis-follower
  labels:
    app: redis
    role: follower
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: follower
    tier: backend
---
# Frontend Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v5
        env:
        - name: GET_HOSTS_FROM
          value: "dns"
        resources:
          requests:
            cpu: 10m
            memory: 10Mi
        ports:
        - containerPort: 80
---
# Frontend Service
apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend
---
# Frontend Ingress
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: guestbook-ingress
  labels:
    app: guestbook
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  rules:
  - host: "guestbook.local"
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend
            port:
              number: 80
EOF

# D√©ployer tout en une fois
kubectl apply -f guestbook-complete.yaml
```

---

## Commandes utiles pour le TP02

### Inspection et debugging

```bash
# Voir tous les objets
kubectl get all

# Voir les objets dans tous les namespaces
kubectl get all -A

# D√©crire un pod
kubectl describe pod <pod-name>

# Logs d'un pod
kubectl logs <pod-name>
kubectl logs -f <pod-name>  # Suivre les logs en temps r√©el
kubectl logs <pod-name> --previous  # Logs du conteneur pr√©c√©dent

# Logs avec s√©lecteur
kubectl logs -l app=code-server -f

# Ex√©cuter une commande dans un pod
kubectl exec -it <pod-name> -- /bin/sh
kubectl exec -it <pod-name> -- bash

# Port-forward pour acc√®s direct
kubectl port-forward pod/<pod-name> 8080:8080
kubectl port-forward service/code-server 8080:8080

# Voir les events
kubectl get events --sort-by='.lastTimestamp'

# Top ressources
kubectl top nodes
kubectl top pods
```

### Gestion des ressources

```bash
# Scaler un deployment
kubectl scale deployment <name> --replicas=5

# √âditer une ressource
kubectl edit deployment <name>

# Mettre √† jour une image
kubectl set image deployment/<name> <container>=<new-image>

# Rollout status
kubectl rollout status deployment/<name>

# Rollback
kubectl rollout undo deployment/<name>

# Historique des rollouts
kubectl rollout history deployment/<name>
```

### Nettoyage

```bash
# Supprimer une ressource
kubectl delete pod <pod-name>
kubectl delete deployment <deployment-name>

# Supprimer via fichier
kubectl delete -f fichier.yaml

# Supprimer toutes les ressources d'un label
kubectl delete all -l app=guestbook

# Supprimer tout dans le namespace
kubectl delete all --all

# Supprimer le namespace (et tout dedans)
kubectl delete namespace tp-kubernetes
```

---

## R√©ponses aux questions du TP

### Q1 : √Ä quoi sert la section env ?
La section `env` permet de passer des variables d'environnement aux conteneurs. Cela permet de configurer l'application sans modifier l'image Docker.

### Q2 : √Ä quoi sert la section volume et volumeMount ?
- `volumes` : D√©clare les volumes disponibles pour le pod
- `volumeMounts` : Monte un volume dans le syst√®me de fichiers du conteneur
- Permet la persistance des donn√©es et le partage entre conteneurs

### Q3 : Pourquoi cr√©er un PVC ?
- Persistance des donn√©es au-del√† du cycle de vie des pods
- Abstraction du stockage physique
- Portabilit√© entre environnements
- S√©paration des responsabilit√©s (dev vs ops)

### Q4 : Diff√©rence entre api-resources namespaced true/false
- **namespaced=true** : Ressources limit√©es √† un namespace (Pod, Service, Deployment)
- **namespaced=false** : Ressources au niveau du cluster (Node, Namespace, PersistentVolume)

Exemples :
```bash
# Ressources namespac√©es
kubectl api-resources --namespaced=true

# Ressources cluster-wide
kubectl api-resources --namespaced=false
```

### Q5 : Comment s√©curiser les secrets ?
**Ne jamais** commiter les secrets en clair dans Git !

Solutions recommand√©es :
1. **Sealed Secrets** (Bitnami) : Chiffre les secrets
2. **External Secrets Operator** : Sync avec des coffres externes
3. **HashiCorp Vault** : Gestion centralis√©e des secrets
4. **Cloud providers** : AWS Secrets Manager, Azure Key Vault, GCP Secret Manager
5. **kubectl create secret** : Cr√©er sans fichier YAML

```bash
# Cr√©er un secret sans fichier
kubectl create secret generic my-secret \
  --from-literal=password=SuperSecret123 \
  --dry-run=client -o yaml | kubectl apply -f -
```

---

## Checklist de validation TP02

### VS Code Server
- [ ] Namespace `tp-kubernetes` cr√©√©
- [ ] Secret cr√©√©
- [ ] PVC cr√©√© et bound
- [ ] Deployment d√©ploy√© avec 1 pod running
- [ ] Service cr√©√©
- [ ] Ingress cr√©√©
- [ ] Application accessible via `http://mon-app.local`
- [ ] Connexion possible avec le mot de passe
- [ ] Logs visibles avec `kubectl logs`
- [ ] Pod se recr√©e automatiquement apr√®s suppression

### Guestbook
- [ ] Redis Leader d√©ploy√© (1 pod)
- [ ] Service Redis Leader cr√©√©
- [ ] Redis Followers d√©ploy√©s (2 pods)
- [ ] Service Redis Followers cr√©√©
- [ ] Frontend d√©ploy√© (3 pods)
- [ ] Service Frontend cr√©√©
- [ ] Ingress cr√©√©
- [ ] Application accessible via `http://guestbook.local`
- [ ] Possibilit√© d'√©crire et lire des messages
- [ ] R√©silience test√©e (suppression de pods)

---

## D√©pannage

### Probl√®me : Erreurs li√©es au proxy

#### Sympt√¥mes
- Pods en √©tat `ImagePullBackOff`
- Erreurs lors de `kubectl apply`
- Timeouts lors du d√©ploiement

#### Solution

```bash
# 1. V√©rifier les variables de proxy shell (DOIVENT √™tre vides)
env | grep -i proxy
# Si quelque chose s'affiche, c'est le probl√®me !

# 2. D√©sactiver toutes les variables de proxy
proxy-off

# OU manuellement :
unset HTTP_PROXY HTTPS_PROXY http_proxy https_proxy NO_PROXY no_proxy
export http_proxy=""
export https_proxy=""

# 3. V√©rifier le proxy Docker (DOIT √™tre configur√©)
sudo systemctl show --property=Environment docker
# Devrait afficher HTTP_PROXY et HTTPS_PROXY

# 4. Si le proxy Docker n'est pas configur√©, le configurer
sudo mkdir -p /etc/systemd/system/docker.service.d
sudo tee /etc/systemd/system/docker.service.d/http-proxy.conf > /dev/null <<'EOF'
[Service]
Environment="HTTP_PROXY=http://proxy.iut.univ:3128"
Environment="HTTPS_PROXY=http://proxy.iut.univ:3128"
Environment="NO_PROXY=localhost,127.0.0.1,.local,.cluster.local"
EOF

sudo systemctl daemon-reload
sudo systemctl restart docker

# 5. Red√©ployer l'application
kubectl delete -f .
kubectl apply -f .
```

### Probl√®me : Pod en √©tat Pending

```bash
# Voir pourquoi
kubectl describe pod <pod-name>

# Causes courantes :
# - PVC non bound
# - Ressources insuffisantes
# - Image pull error
```

### Probl√®me : Ingress ne fonctionne pas

```bash
# V√©rifier l'Ingress Controller
kubectl get pods -n ingress-nginx

# V√©rifier les logs
kubectl logs -n ingress-nginx -l app.kubernetes.io/component=controller

# V√©rifier la configuration Ingress
kubectl describe ingress <ingress-name>

# V√©rifier /etc/hosts
cat /etc/hosts | grep local
```

### Probl√®me : PVC reste en Pending

```bash
# Voir les d√©tails
kubectl describe pvc <pvc-name>

# V√©rifier les StorageClass
kubectl get storageclass

# Kind devrait avoir une StorageClass par d√©faut
```

### Probl√®me : Image pull error

```bash
# Voir les d√©tails
kubectl describe pod <pod-name>

# Causes :
# - Image n'existe pas
# - Probl√®me de proxy
# - Probl√®me de registry
```

---

## Nettoyage complet

```bash
# Supprimer VS Code Server
cd ~/tp02/vs_code
kubectl delete -f .

# Supprimer Guestbook
cd ~/tp02/guestbook-php
kubectl delete -f .

# Supprimer le namespace (supprime tout dedans)
kubectl delete namespace tp-kubernetes

# Recr√©er le namespace pour un nouveau test
kubectl create namespace tp-kubernetes
kubectl config set-context --current --namespace=tp-kubernetes
```

---

## R√©sum√© : Configuration Proxy pour TP02

### Checklist Proxy avant de commencer

```bash
# ‚úÖ 1. Proxy Docker daemon configur√©
sudo systemctl show --property=Environment docker | grep PROXY
# Doit afficher : HTTP_PROXY et HTTPS_PROXY

# ‚ùå 2. Variables proxy shell D√âSACTIV√âES
env | grep -i proxy
# Ne doit RIEN afficher

# ‚úÖ 3. Test de pull d'image Docker
docker pull nginx:latest
# Doit fonctionner

# ‚úÖ 4. Test kubectl
kubectl get nodes
# Doit afficher les n≈ìuds du cluster
```

### Aide-m√©moire commandes proxy

```bash
# Activer le proxy (rarement n√©cessaire pendant le TP)
proxy-on

# D√©sactiver le proxy (toujours pour Kubernetes)
proxy-off

# V√©rifier l'√©tat
proxy-status
```

### Si rien ne fonctionne

```bash
# Reset complet de la configuration proxy
# 1. D√©sactiver toutes les variables
unset HTTP_PROXY HTTPS_PROXY http_proxy https_proxy NO_PROXY no_proxy

# 2. V√©rifier Docker
sudo systemctl restart docker
sudo systemctl show --property=Environment docker

# 3. Recr√©er le cluster Kind si n√©cessaire
kind delete cluster --name tp-cluster
kind create cluster --name tp-cluster --config kind-cluster-config.yaml

# 4. R√©installer l'Ingress Controller
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml
```

---

## Ressources compl√©mentaires

- [Guide d'installation TP01](INSTALL_TP1_RAPIDE.md) - **Contient la configuration proxy compl√®te**
- [Glossaire Kubernetes](KUBERNETES_GLOSSAIRE.md)
- [Documentation Kubernetes](https://kubernetes.io/docs/)
- [Documentation Kind](https://kind.sigs.k8s.io/)
- [Ingress NGINX](https://kubernetes.github.io/ingress-nginx/)

---

## Compte-rendu du TP02

**N'oubliez pas de faire votre compte-rendu avec :**

### Contenu obligatoire
- ‚úÖ Captures d'√©cran des applications fonctionnelles
  - VS Code Server accessible via navigateur
  - Guestbook accessible et fonctionnel
- ‚úÖ R√©ponses aux questions marqu√©es üôã
- ‚úÖ Explications de vos choix techniques
- ‚úÖ Probl√®mes rencontr√©s et solutions appliqu√©es

### Questions √† r√©pondre

1. **√Ä quoi sert la section env ?**
2. **√Ä quoi sert la section volume et volumeMount ?**
3. **Pourquoi cr√©er un PVC ?**
4. **Comment acc√©der √† l'application mon-app.local ?**
5. **Comment afficher les logs des requ√™tes entrantes ?**
6. **Que se passe-t-il quand vous supprimez un pod ?**
7. **Comment s√©curiser les secrets ?**
8. **Diff√©rence entre api-resources namespaced true/false ?**

### Bonus : Probl√®mes proxy rencontr√©s

Si vous avez rencontr√© des probl√®mes li√©s au proxy, documentez :
- Le sympt√¥me observ√©
- Comment vous avez diagnostiqu√© le probl√®me
- La solution appliqu√©e
- Comment v√©rifier que c'est r√©solu

---

**F√©licitations !** üéâ

Vous avez maintenant d√©ploy√© deux applications compl√®tes sur Kubernetes et ma√Ætrisez les concepts de base du d√©ploiement d'applications conteneuris√©es.

**Points cl√©s acquis** :
- ‚úÖ D√©ploiement d'applications stateless et stateful
- ‚úÖ Gestion du stockage persistant
- ‚úÖ Exposition via Services et Ingress
- ‚úÖ Gestion des Secrets
- ‚úÖ Architecture multi-tiers
- ‚úÖ **Configuration proxy pour environnement IUT**
- ‚úÖ Debugging et d√©pannage
